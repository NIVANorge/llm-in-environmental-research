{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:37:12.627079Z",
     "start_time": "2025-12-04T15:37:12.303989Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# === 1. Load the data ===\n",
    "\n",
    "# Adjust the path if needed (e.g. \"data/lump.xlsx\")\n",
    "FILE_PATH = \"lump.xlsx\""
   ],
   "id": "6e9108a392d0743e",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-12-04T15:26:47.390937Z",
     "start_time": "2025-12-04T15:26:47.366848Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "# Normalize column names a bit (strip spaces)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "# Identify columns\n",
    "reviewer_col = \"Reviewers verdict\"\n",
    "\n",
    "# All LLM columns are the verdict columns except the reviewer one and Paper ID\n",
    "llm_cols = [\n",
    "    c for c in df.columns\n",
    "    if c not in (\"Paper ID\", reviewer_col)\n",
    "]\n",
    "\n",
    "print(\"LLM columns detected:\", llm_cols)\n",
    "\n",
    "# Make everything lowercase strings for safety\n",
    "df[reviewer_col] = df[reviewer_col].astype(str).str.strip().str.lower()\n",
    "for c in llm_cols:\n",
    "    df[c] = df[c].astype(str).str.strip().str.lower()\n",
    "\n",
    "\n",
    "# === 2. Per-model agreement with reviewers ===\n",
    "\n",
    "print(\"\\n=== Per-model agreement with reviewers ===\")\n",
    "for model_col in llm_cols:\n",
    "    agrees = (df[model_col] == df[reviewer_col])\n",
    "    accuracy = agrees.mean()\n",
    "    print(f\"{model_col}: {accuracy:.3f} agreement with reviewers \"\n",
    "          f\"({agrees.sum()}/{len(df)})\")\n",
    "\n",
    "\n",
    "# === 3. Majority consensus among LLMs and agreement with reviewers ===\n",
    "\n",
    "def compute_majority_vote(row, model_columns):\n",
    "    \"\"\"\n",
    "    Given a row and list of LLM columns, return:\n",
    "      - majority_label (str) if a strict majority exists\n",
    "      - None otherwise\n",
    "    \"\"\"\n",
    "    votes = [row[c] for c in model_columns]\n",
    "    # Remove empty/NA-like strings if any\n",
    "    votes = [v for v in votes if isinstance(v, str) and v != \"\"]\n",
    "    if not votes:\n",
    "        return None\n",
    "\n",
    "    counts = Counter(votes)\n",
    "    label, count = counts.most_common(1)[0]\n",
    "    if count > len(votes) / 2.0:\n",
    "        return label  # strict majority\n",
    "    return None      # no strict majority\n",
    "\n",
    "\n",
    "# Compute majority vote for each row\n",
    "df[\"LLM_majority\"] = df.apply(\n",
    "    lambda row: compute_majority_vote(row, llm_cols),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[\"Has_majority\"] = df[\"LLM_majority\"].notna()\n",
    "\n",
    "# Compare majority vote to reviewers’ verdict\n",
    "df[\"Majority_agrees_with_reviewer\"] = (\n",
    "    df[\"Has_majority\"] &\n",
    "    (df[\"LLM_majority\"] == df[reviewer_col])\n",
    ")\n",
    "\n",
    "# === 4. Summary statistics ===\n",
    "\n",
    "total_papers = len(df)\n",
    "with_majority = df[\"Has_majority\"].sum()\n",
    "\n",
    "print(\"\\n=== Majority consensus vs reviewers ===\")\n",
    "print(f\"Total papers: {total_papers}\")\n",
    "print(f\"Papers with an LLM majority decision: {with_majority} \"\n",
    "      f\"({with_majority / total_papers:.3f})\")\n",
    "\n",
    "if with_majority > 0:\n",
    "    agrees_count = df[\"Majority_agrees_with_reviewer\"].sum()\n",
    "    print(\n",
    "        f\"Among papers with an LLM majority:\\n\"\n",
    "        f\"  Majority agrees with reviewers on {agrees_count}/{with_majority} \"\n",
    "        f\"({agrees_count / with_majority:.3f})\"\n",
    "    )\n",
    "else:\n",
    "    print(\"No rows had a strict majority among LLMs.\")\n",
    "\n",
    "# Optional: inspect a few rows where majority disagrees\n",
    "disagreements = df[\n",
    "    df[\"Has_majority\"] &\n",
    "    (df[\"LLM_majority\"] != df[reviewer_col])\n",
    "]\n",
    "\n",
    "print(\"\\nExample rows where LLM majority disagrees with reviewers:\")\n",
    "print(disagreements.head())\n"
   ],
   "id": "4aac28b1d8cb69ff",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM columns detected: ['ChatGPT verdict', 'Claude verdict', 'DeepSeek verdict', 'Gemini verdict', 'Mistral verdict']\n",
      "\n",
      "=== Per-model agreement with reviewers ===\n",
      "ChatGPT verdict: 0.868 agreement with reviewers (434/500)\n",
      "Claude verdict: 0.868 agreement with reviewers (434/500)\n",
      "DeepSeek verdict: 0.954 agreement with reviewers (477/500)\n",
      "Gemini verdict: 0.844 agreement with reviewers (422/500)\n",
      "Mistral verdict: 0.706 agreement with reviewers (353/500)\n",
      "\n",
      "=== Majority consensus vs reviewers ===\n",
      "Total papers: 500\n",
      "Papers with an LLM majority decision: 500 (1.000)\n",
      "Among papers with an LLM majority:\n",
      "  Majority agrees with reviewers on 433/500 (0.866)\n",
      "\n",
      "Example rows where LLM majority disagrees with reviewers:\n",
      "   Paper ID Reviewers verdict ChatGPT verdict Claude verdict DeepSeek verdict  \\\n",
      "1         7           exclude         exclude        include          exclude   \n",
      "2        13           include         exclude        exclude          exclude   \n",
      "4        22           exclude         include        include          include   \n",
      "5        34           exclude         include        exclude          exclude   \n",
      "7        45           include         include        exclude          exclude   \n",
      "\n",
      "  Gemini verdict Mistral verdict LLM_majority  Has_majority  \\\n",
      "1        include         include      include          True   \n",
      "2        exclude         exclude      exclude          True   \n",
      "4        include         include      include          True   \n",
      "5        include         include      include          True   \n",
      "7        exclude         include      exclude          True   \n",
      "\n",
      "   Majority_agrees_with_reviewer  \n",
      "1                          False  \n",
      "2                          False  \n",
      "4                          False  \n",
      "5                          False  \n",
      "7                          False  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "dedd83018ed58e60"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "b3f508db8a8fc350"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:37:15.304623Z",
     "start_time": "2025-12-04T15:37:15.286337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "df = pd.read_excel(FILE_PATH)\n",
    "\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "reviewer_col = \"Reviewers verdict\"\n",
    "llm_cols = [c for c in df.columns if c not in (\"Paper ID\", reviewer_col)]\n",
    "\n",
    "# Normalize\n",
    "df[reviewer_col] = df[reviewer_col].str.lower().str.strip()\n",
    "for c in llm_cols:\n",
    "    df[c] = df[c].str.lower().str.strip()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "WEIGHTS = {\n",
    "    \"DeepSeek verdict\": {\"include\": 1.0, \"exclude\": 3.0},\n",
    "    \"Mistral verdict\":  {\"include\": 3.0, \"exclude\": 1.0},\n",
    "\n",
    "    # Balanced models → weight both classes equally\n",
    "    \"ChatGPT verdict\":  {\"include\": 1.0, \"exclude\": 1.0},\n",
    "    \"Claude verdict\":   {\"include\": 1.0, \"exclude\": 1.0},\n",
    "    \"Gemini verdict\":   {\"include\": 1.0, \"exclude\": 1.0},\n",
    "}\n",
    "\n",
    "\n",
    "# === 3. Weighted voting function ===\n",
    "def weighted_vote(row, llm_columns, weight_table):\n",
    "    include_score = 0.0\n",
    "    exclude_score = 0.0\n",
    "\n",
    "    for model in llm_columns:\n",
    "        verdict = row[model]\n",
    "        if verdict == \"include\":\n",
    "            include_score += weight_table[model][\"include\"]\n",
    "        elif verdict == \"exclude\":\n",
    "            exclude_score += weight_table[model][\"exclude\"]\n",
    "\n",
    "    if include_score > exclude_score:\n",
    "        return \"include\"\n",
    "    elif exclude_score > include_score:\n",
    "        return \"exclude\"\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "\n",
    "# === 4. Compute weighted consensus ===\n",
    "df[\"Weighted_majority\"] = df.apply(\n",
    "    lambda r: weighted_vote(r, llm_cols, WEIGHTS),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df[\"Weighted_agrees_with_reviewer\"] = (\n",
    "    df[\"Weighted_majority\"] == df[reviewer_col]\n",
    ")\n",
    "\n",
    "\n",
    "# === 5. Summary ===\n",
    "total = len(df)\n",
    "agree = df[\"Weighted_agrees_with_reviewer\"].sum()\n",
    "\n",
    "print(\"\\n=== Weighted Majority Accuracy ===\")\n",
    "print(f\"Weighted majority agrees with reviewers on {agree}/{total} \"\n",
    "      f\"({agree/total:.3f})\")\n",
    "\n",
    "print(\"\\n=== Rows where weighted majority disagrees with reviewers ===\")\n",
    "print(df[df[\"Weighted_majority\"] != df[reviewer_col]].head())"
   ],
   "id": "46003ea073f20ab4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Weighted Majority Accuracy ===\n",
      "Weighted majority agrees with reviewers on 433/500 (0.866)\n",
      "\n",
      "=== Rows where weighted majority disagrees with reviewers ===\n",
      "   Paper ID Reviewers verdict ChatGPT verdict Claude verdict DeepSeek verdict  \\\n",
      "1         7           exclude         exclude        include          exclude   \n",
      "2        13           include         exclude        exclude          exclude   \n",
      "4        22           exclude         include        include          include   \n",
      "5        34           exclude         include        exclude          exclude   \n",
      "7        45           include         include        exclude          exclude   \n",
      "\n",
      "  Gemini verdict Mistral verdict Weighted_majority  \\\n",
      "1        include         include           include   \n",
      "2        exclude         exclude           exclude   \n",
      "4        include         include           include   \n",
      "5        include         include           include   \n",
      "7        exclude         include           exclude   \n",
      "\n",
      "   Weighted_agrees_with_reviewer  \n",
      "1                          False  \n",
      "2                          False  \n",
      "4                          False  \n",
      "5                          False  \n",
      "7                          False  \n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "1f9ad02463de2db3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "579fded51aaad9a0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2111ebb3cfdfcc0b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-04T15:49:22.194426Z",
     "start_time": "2025-12-04T15:49:12.329568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# === 1. Load data ===\n",
    "df = pd.read_excel(FILE_PATH)\n",
    "df.columns = [c.strip() for c in df.columns]\n",
    "\n",
    "reviewer_col = \"Reviewers verdict\"\n",
    "llm_cols = [c for c in df.columns if c not in (\"Paper ID\", reviewer_col)]\n",
    "\n",
    "# Clean values\n",
    "def clean(v):\n",
    "    if pd.isna(v): return \"\"\n",
    "    return str(v).strip().lower()\n",
    "\n",
    "df[reviewer_col] = df[reviewer_col].apply(clean)\n",
    "for c in llm_cols:\n",
    "    df[c] = df[c].apply(clean)\n",
    "\n",
    "# Convert include/exclude to 1/0\n",
    "mapping = {\"include\": 1, \"exclude\": 0}\n",
    "y = df[reviewer_col].map(mapping)\n",
    "\n",
    "X = df[llm_cols].applymap(lambda v: mapping.get(v, 0))\n",
    "\n",
    "\n",
    "# === 2. Fit logistic regression to learn optimal weights ===\n",
    "model = LogisticRegression()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Extract learned optimal weights\n",
    "weights = dict(zip(llm_cols, model.coef_[0]))\n",
    "bias = model.intercept_[0]\n",
    "\n",
    "print(\"\\n=== Optimal Weights Learned from Data ===\")\n",
    "for col, w in weights.items():\n",
    "    print(f\"{col:20s}  weight = {w:.3f}\")\n",
    "\n",
    "print(f\"Bias term: {bias:.3f}\")\n",
    "\n",
    "\n",
    "# === 3. Predict using optimized ensemble ===\n",
    "y_pred = model.predict(X)\n",
    "accuracy = accuracy_score(y, y_pred)\n",
    "\n",
    "print(\"\\n=== Accuracy of Optimal Weighted Ensemble ===\")\n",
    "print(f\"Accuracy: {accuracy:.3f}  ({sum(y_pred==y)}/{len(y)})\")\n",
    "\n",
    "\n",
    "# Optional: Show where the model disagrees with reviewers\n",
    "disagreements = df[y_pred != y]\n",
    "print(\"\\nRows where optimized ensemble disagrees with reviewers:\")\n",
    "print(disagreements.head())\n"
   ],
   "id": "9f5eea5681555cef",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Optimal Weights Learned from Data ===\n",
      "ChatGPT verdict       weight = 0.823\n",
      "Claude verdict        weight = 0.760\n",
      "DeepSeek verdict      weight = 0.478\n",
      "Gemini verdict        weight = 0.026\n",
      "Mistral verdict       weight = 1.045\n",
      "Bias term: -4.459\n",
      "\n",
      "=== Accuracy of Optimal Weighted Ensemble ===\n",
      "Accuracy: 0.968  (484/500)\n",
      "\n",
      "Rows where optimized ensemble disagrees with reviewers:\n",
      "    Paper ID Reviewers verdict ChatGPT verdict Claude verdict  \\\n",
      "2         13           include         exclude        exclude   \n",
      "3         15           include         include        include   \n",
      "7         45           include         include        exclude   \n",
      "57       237           include         include        include   \n",
      "69       279           include         exclude        exclude   \n",
      "\n",
      "   DeepSeek verdict Gemini verdict Mistral verdict  \n",
      "2           exclude        exclude         exclude  \n",
      "3           include        include         include  \n",
      "7           exclude        exclude         include  \n",
      "57          exclude        include         include  \n",
      "69          exclude        exclude         exclude  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p5/s7tqqff5719cbgj_thn4d_jm0000gn/T/ipykernel_40750/1184226402.py:25: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  X = df[llm_cols].applymap(lambda v: mapping.get(v, 0))\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "305a2ae738ef0bc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
